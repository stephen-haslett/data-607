---
title: "DATA 607 Project 4 - Document Classification"
author: "Stephen Haslett"
date: "10/28/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, eval=TRUE, message=FALSE, warning=FALSE}
#@TODO WEED OUT UNUSED LIBRARIES!!!!
library(tm)
library(knitr)
library(dplyr)
library(tidytext)
library(ggplot2)
library(kableExtra)
library(wordcloud)
library(tidyverse)
library(readtext)
library(caTools)
library(randomForest)
library(e1071)
library(caret)

spam_directory <- "/Users/stephenhaslett/Data607/data_607_project_4/spam_ham_training/spam"
ham_directory <- "/Users/stephenhaslett/Data607/data_607_project_4/spam_ham_training/easy_ham"
```

## Assignment Overview
It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder). One example corpus: https://spamassassin.apache.org/old/publiccorpus/

####List the first 6 Ham training files contained within the "spam_ham_training/easy_ham" directory.

```{r, eval=TRUE, message=FALSE, warning=FALSE}
ham_training_files <- list.files(ham_directory)
head(ham_training_files)
```


####List the first 6 Spam training files contained within the "spam_ham_training/spam" directory.

```{r, eval=TRUE, message=FALSE, warning=FALSE}
spam_training_files <- list.files(spam_directory)
head(spam_training_files)
```

### Define a function that removes email headers so that we are left with clean data for analysis.
```{r, eval=TRUE, message=FALSE, warning=FALSE}
#' Strips header data from emails.
#'
#' Removes irrelevant header information from emails so we
#' are left with clean body text for accurate test/training comparison.
#'
#' @param email Email content.
#'
#' @return Email body content free from header data.
#'
strip_email_headers <- function(email) {
  message <- str_split(email,"\n\n") %>% unlist()
  email_body_content <- paste(message[2:length(message)], collapse = ' ')

  return(email_body_content)
}
```

### Define a function that retrieves data from the training files and returns a data frame that we can use for further analysis.
```{r, eval=TRUE, message=FALSE, warning=FALSE}
#' Retrieves data from spam and ham training files.
#'
#' Pulls in email training data from the given directory, cleans the data for analysis,
#' and returns it as a data frame.
#'
#' @param directory String: The path to the ham/spam training files directory.
#' @param type String: The type of email data being passed.
#'
#' @return A data frame of email training file data.
#'
fetch_training_file_data <- function(directory, type) {
  training_files <- list.files(directory)
  message_content <- NA
  count <- 1

  for (file in 1:length(training_files)) {
    file_path <- paste0(directory, '/', training_files[file])
    training_email <-suppressWarnings(warning(readtext(file_path, TRUE)))
    training_email_body <- strip_email_headers(training_email)
    training_email_body <- gsub("<.*?>", " ", training_email_body)
    message <- list(paste(training_email_body, collapse = '\n'))
    message_content <- c(message_content, message)
    count <- count + 1
  }
  
  # When we perform our prediction analysis of the data, the randomForest() function expects a
  # numeric label, so we create a new column called "document_class", and populate it with
  # numeric values based on the value of the "type" parameter (0 for ham, 1 for spam).
  if (type == 'ham') {
    document_class <- 0
  } else if (type == 'spam') {
    document_class <- 1
  }

  training_emails <- data.frame()
  training_emails <- as.data.frame(unlist(message_content), stringsAsFactors = FALSE)
  training_emails$message_type <- type
  training_emails$document_class <- document_class
  colnames(training_emails) <- c('message', 'message_type', 'document_class')
 
  return (training_emails)
}
```


```{r, eval=TRUE, message=FALSE, warning=FALSE}
#' Converts spam/ham data frames to corpuses.
#'
#' Pulls in an email training dataframe, cleans the data,
#' and returns it as a corpus.
#'
#' @param email_data Dataframe: A dataframe containing ham/spam email data.
#' @param type String: The type of email data being passed.
#'
#' @return A corpus of training email data.
#'
create_training_corpus <- function(email_data, type) {
  # Create a corpus of email content and clean the data for analysis purposes.
  training_corpus <- VCorpus(VectorSource(email_data))
  
  if (type != 'combined') {
    meta(training_corpus, tag = 'messageType') <- type
  }

  training_corpus <- tm_map(training_corpus, content_transformer(function(x) iconv(x, "UTF-8", "ASCII")))
  training_corpus <- tm_map(training_corpus, content_transformer(tolower))
  training_corpus <- tm_map(training_corpus, stripWhitespace)
  training_corpus <- tm_map(training_corpus, PlainTextDocument)
  training_corpus <- tm_map(training_corpus, removePunctuation)
  training_corpus <- tm_map(training_corpus, removeNumbers)
  training_corpus <- tm_map(training_corpus, content_transformer(removeWords), stopwords("english"))   
  training_corpus <- tm_map(training_corpus, removeWords, stopwords("english")) 

  return(training_corpus)
}
```


###Output the intial Raw count of Ham term frequencies.
```{r, eval=TRUE, message=FALSE, warning=FALSE}
# Create the Ham dataframe.
ham_directory <- "/Users/stephenhaslett/Data607/data_607_project_4/spam_ham_training/easy_ham"
ham_dataframe <- fetch_training_file_data(ham_directory, 'ham')

# Create the ham corpus and term matrix.
ham_corpus <- create_training_corpus(ham_dataframe, 'ham')
  
ham_term_matrix <- DocumentTermMatrix(ham_corpus)
ham_term_matrix <- removeSparseTerms(ham_term_matrix, 0.99)
ham_ordered <- as.matrix(ham_term_matrix)
ham_term_frequency <- colSums(ham_ordered)
ham_term_frequency <- sort(ham_term_frequency, decreasing = T)
ham_data <- head(ham_term_frequency, 35)
kable(ham_data, "html", escape = F) %>%
  kable_styling("striped", full_width = T) %>%
  column_spec(1, bold = T)
```


###Bar plot for ham.
```{r, message=FALSE, warning=FALSE}
ham_term_freq <- data.frame(Term = names(ham_term_frequency), Frequency = ham_term_frequency)
ham_term_plot <- ggplot(subset(ham_term_freq, Frequency > 500), aes(x = reorder(Term, - Frequency), y = Frequency)) +
  geom_bar(stat = "identity", fill = '#4CAF50') +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  theme(panel.background = element_rect(fill = '#FFFFFF'))

ham_term_plot
```

###Ham Word Cloud
```{r, eval=TRUE, message=FALSE, warning=FALSE}
ham_term_frequency[1:50]
ham_terms <- names(ham_term_frequency)
wordcloud(ham_terms[1:50], ham_term_frequency[1:50], random.color = TRUE, colors = palette())
```


```{r, eval=TRUE, message=FALSE, warning=FALSE}
# Create the Spam dataframe.
spam_directory <- "/Users/stephenhaslett/Data607/data_607_project_4/spam_ham_training/spam"
spam_dataframe <- fetch_training_file_data(spam_directory, 'spam')

# Create the spam corpus and term matrix.
spam_corpus <- create_training_corpus(spam_dataframe, 'spam')

spam_term_matrix <- DocumentTermMatrix(spam_corpus)
spam_term_matrix <- removeSparseTerms(spam_term_matrix, 0.99)
spam_ordered <- as.matrix(spam_term_matrix)
spam_term_frequency <- colSums(spam_ordered)
spam_term_frequency <- sort(spam_term_frequency, decreasing = T)
spam_data <- head(spam_term_frequency, 35)
kable(spam_data, "html", escape = F) %>%
  kable_styling("striped", full_width = T) %>%
  column_spec(1, bold = T)
```

###Bar plot for spam.
```{r, message=FALSE, warning=FALSE}
spam_term_freq <- data.frame(Term = names(spam_term_frequency), Frequency = spam_term_frequency)
spam_term_plot <- ggplot(subset(spam_term_freq, Frequency > 500), aes(x = reorder(Term, - Frequency), y = Frequency)) +
  geom_bar(stat = "identity", fill = '#DC143C') +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  theme(panel.background = element_rect(fill = '#FFFFFF'))

spam_term_plot
```


##Spam Word Cloud
```{r, eval=TRUE, message=FALSE, warning=FALSE}
spam_term_frequency[1:50]
spam_terms <- names(spam_term_frequency)
wordcloud(spam_terms[1:50], spam_term_frequency[1:50], random.color = TRUE, colors = palette())
```

###Merge the Spam and Ham dataframes together.
```{r, eval=TRUE, message=FALSE, warning=FALSE}
combined_dataframe <- rbind(spam_dataframe, ham_dataframe)
combined_corpus <- create_training_corpus(combined_dataframe$message, 'combined') 

combined_term_matrix <- DocumentTermMatrix(combined_corpus)
combined_term_matrix <- removeSparseTerms(combined_term_matrix, 0.99)

combined_dataset = as.data.frame(as.matrix(combined_term_matrix))
combined_dataset$document_class <- combined_dataframe$document_class

```

###Randomize the dataset and split it into 2 seperate datasets - Training and Testing.
```{r, eval=TRUE, message=FALSE, warning=FALSE}
randomized_dataset <- combined_dataset[sample(c(1:length(combined_dataset)))]

split <- sample.split(randomized_dataset$document_class, SplitRatio = 0.8)
training_dataset <- subset(randomized_dataset, split == TRUE)
testing_dataset = subset(randomized_dataset, split == FALSE)
```

##Training and Testing
```{r, eval=TRUE, message=FALSE, warning=FALSE}
sample_size <- ncol(training_dataset) -1
classification <- randomForest(x = training_dataset[-sample_size], y = training_dataset$document_class, ntree = 3)

spam_prediction = predict(classification, newdata = testing_dataset[-sample_size])

confusion_matrix <- table(spam_prediction > 0, testing_dataset$document_class)
colnames(confusion_matrix) <- c('HAM', 'SPAM')

confusion_matrix
```


##Calculate the accuracy of the model.
```{r, eval=TRUE, message=FALSE, warning=FALSE}
success <- confusion_matrix['TRUE', 2] + confusion_matrix['FALSE', 1]
accuracy_percentage <- success / nrow(testing_dataset) * 100
accuracy_percentage
```